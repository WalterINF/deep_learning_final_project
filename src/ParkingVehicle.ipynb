{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e72eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imageio imageio-ffmpeg pygame numerize casadi stable-baselines3 tensorboard \"stable-baselines3[extra]\"  pyvirtualdisplay ipywidgets --quiet\n",
    "%pip install \"gymnasium[other]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import stable_baselines3\n",
    "from SimulationConfigLoader import VehicleConfigLoader, MapConfigLoader\n",
    "from Simulation import ArticulatedVehicle\n",
    "from Simulation import Map\n",
    "from Simulation import MapEntity\n",
    "import random\n",
    "import Visualization\n",
    "from casadi import cos, sin, tan\n",
    "from typing import Any, SupportsFloat\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "from numerize import numerize\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "import platform\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77f3d1",
   "metadata": {},
   "source": [
    "#### API Gymnasium\n",
    "\n",
    "Aqui é definido o ambiente de treinamento, extendendo a classe gym.Env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0830402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParkingEnv(gym.Env):\n",
    "\n",
    "    metadata = {\"render_modes\": [\"rgb_array\"], \"render_fps\": 24}\n",
    "\n",
    "\n",
    "    ## ambiente\n",
    "    VEHICLE_NAME = \"BUG1\" #nome do veículo\n",
    "    MAP_NAME = \"MAPA_1\" #nome do mapa\n",
    "    SENSOR_RANGE_M = 50.0 # raio do sensor\n",
    "    SPEED_LIMIT_MS = 5.0 # velocidade maxima\n",
    "    STEERING_LIMIT_RAD = float(np.deg2rad(28.0)) # angulo maximo de esterçamento\n",
    "    JACKKNIFE_LIMIT_RAD = float(np.deg2rad(65.0)) # angulo maximo de jackknife\n",
    "    DT = 0.5 # tempo de simulação\n",
    "    MAX_SECONDS = 120.0\n",
    "    MAX_STEPS = int(MAX_SECONDS / DT)\n",
    "\n",
    "\n",
    "    ## recompensas\n",
    "    MAX_PUNISHMENT_TIME_PER_EPISODE = -20.0 # penalidade maxima por tempo acumulada por episodio\n",
    "    #deve ser menor que a punição por colisão e jackknife, senão o agente vai colidir propositalmente\n",
    "\n",
    "    REWARD_GOAL = 100.0 # recompensa por chegar ao objetivo\n",
    "    PUNISHMENT_TIME = MAX_PUNISHMENT_TIME_PER_EPISODE / MAX_STEPS # penalidade por tempo a cada passo\n",
    "    PUNISHMENT_ZERO_SPEED = 5 * PUNISHMENT_TIME # penalidade por velocidade zero - 5 vezes maior que a penalidade por tempo\n",
    "    PUNISHMENT_COLLISION = -150.0 # penalidade por colisão\n",
    "    PUNISHMENT_JACKKNIFE = -150.0 # penalidade por jackknife\n",
    "    PROGRESS_REWARD_MULTIPLIER = 0.1 # multiplicador da recompensa por progresso (recompensa = metros ganhos * multiplicador)\n",
    "\n",
    "    def __init__(self, seed = 0):\n",
    "\n",
    "        self.render_mode = \"rgb_array\"\n",
    "\n",
    "        self.vehicle_loader = VehicleConfigLoader(\"config/lista_veiculos.json\")\n",
    "        self.map_loader = MapConfigLoader(\"config/lista_mapas.json\")\n",
    "\n",
    "        self.vehicle = self.vehicle_loader.load_vehicle(self.VEHICLE_NAME)\n",
    "        self.map = self.map_loader.load_map(self.MAP_NAME)\n",
    "\n",
    "        self.map.place_vehicle(self.vehicle)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        self.last_distance_to_goal = self._calculate_goal_distance_manhattan()\n",
    "\n",
    "        \n",
    "        # Gymnasium spaces based on README specifications\n",
    "\n",
    "\n",
    "        # Observation: [x, y, theta, beta, alpha, r1..r14, goal_x, goal_y, goal_theta]\n",
    "        obs_low = np.array(\n",
    "            [\n",
    "                -np.pi,                      # theta\n",
    "                -self.JACKKNIFE_LIMIT_RAD,        # beta\n",
    "                -self.STEERING_LIMIT_RAD,         # alpha\n",
    "            ]\n",
    "            + [0.0] * self.vehicle.get_raycast_count()                     # raycasts\n",
    "            + [0.0, -np.pi],            # goal distance, goal direction\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        obs_high = np.array(\n",
    "            [\n",
    "                np.pi,                       # theta\n",
    "                self.JACKKNIFE_LIMIT_RAD,         # beta\n",
    "                self.STEERING_LIMIT_RAD,          # alpha\n",
    "            ]\n",
    "            + [self.SENSOR_RANGE_M] * self.vehicle.get_raycast_count()          # raycasts\n",
    "            + [100, np.pi], # goal distance, goal direction\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "\n",
    "        # Action: [v, alpha]\n",
    "        act_low = np.array(\n",
    "            [\n",
    "                -self.SPEED_LIMIT_MS,             # v (allow reverse)\n",
    "                -self.STEERING_LIMIT_RAD,         # alpha\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        act_high = np.array(\n",
    "            [\n",
    "                self.SPEED_LIMIT_MS,             # v (allow reverse)\n",
    "                self.STEERING_LIMIT_RAD,          # alpha\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.action_space = gym.spaces.Box(low=act_low, high=act_high, dtype=np.float32)\n",
    "\n",
    "    def reset(self, *, seed: int | None = None, options: dict | None = None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.vehicle = self.vehicle_loader.load_vehicle(self.VEHICLE_NAME)\n",
    "        self.map = self.map_loader.load_map(self.MAP_NAME)\n",
    "\n",
    "        self.map.place_vehicle(self.vehicle)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        self.last_distance_to_goal = self._calculate_goal_distance_manhattan()\n",
    "\n",
    "        # Build observation\n",
    "        theta = self.vehicle.get_theta()\n",
    "        beta = self.vehicle.get_beta()\n",
    "        alpha_current = self.vehicle.get_alpha()\n",
    "        raycast_lengths = self.vehicle.get_raycast_lengths()\n",
    "        goal_distance = self._calculate_goal_distance()\n",
    "        goal_direction = self._calculate_goal_direction()\n",
    "        observation = np.array([theta, beta, alpha_current] + raycast_lengths + [goal_distance, goal_direction], dtype=np.float32)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action) -> tuple[np.ndarray, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        velocity, alpha = action\n",
    "        self.steps += 1\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        reward = self.PUNISHMENT_TIME # recompensa base por passo de tempo (reduzida)\n",
    "\n",
    "        self._move_vehicle(velocity, alpha, self.DT)\n",
    "\n",
    "        ## Strongly punish zero/low speed\n",
    "        if abs(velocity) < 0.1:\n",
    "            reward = self.PUNISHMENT_ZERO_SPEED  # Penalidade muito maior para ficar parado\n",
    "\n",
    "        if self.steps >= self.MAX_STEPS:\n",
    "            truncated = True\n",
    "        if self._check_vehicle_parking():\n",
    "            terminated = True\n",
    "            reward = self.REWARD_GOAL\n",
    "        elif self._check_vehicle_collision():\n",
    "            terminated = True\n",
    "            reward = self.PUNISHMENT_COLLISION\n",
    "        elif self._check_trailer_jackknife():\n",
    "            terminated = True\n",
    "            reward = self.PUNISHMENT_JACKKNIFE\n",
    "\n",
    "        new_distance_to_goal = self._calculate_goal_distance_manhattan()\n",
    "\n",
    "        # Recompensa baseada no progresso real\n",
    "        progress_reward = (self.last_distance_to_goal - new_distance_to_goal)\n",
    "        reward += progress_reward * self.PROGRESS_REWARD_MULTIPLIER # Ajuste o multiplicador (0.1) conforme necessário\n",
    "\n",
    "        self.last_distance_to_goal = new_distance_to_goal\n",
    "\n",
    "        # Observação: [x, y, theta, beta, alpha, r1..r14, goal_dist, goal_direction]\n",
    "        theta = self.vehicle.get_theta()\n",
    "        beta = self.vehicle.get_beta()\n",
    "        alpha_current = self.vehicle.get_alpha()\n",
    "        raycast_lengths = self.vehicle.get_raycast_lengths()\n",
    "        observation = np.array([theta, beta, alpha_current] + raycast_lengths + [self._calculate_goal_distance(), self._calculate_goal_direction()], dtype=np.float32)\n",
    "\n",
    "        self.total_reward += reward\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        rgb_array = Visualization.to_rgb_array(self.map, self.vehicle, (288, 288), self._calculate_goal_distance(), self._calculate_goal_direction(), total_reward=self.total_reward)\n",
    "        return rgb_array\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _move_vehicle(self, velocity: float, alpha: float, dt: float):\n",
    "        # Current state\n",
    "        x, y = self.vehicle.get_position()\n",
    "        theta = self.vehicle.get_theta()\n",
    "        beta = self.vehicle.get_beta()\n",
    "\n",
    "        # Geometry\n",
    "        D = self.vehicle.get_distancia_eixo_dianteiro_quinta_roda() - self.vehicle.get_distancia_eixo_traseiro_quinta_roda()\n",
    "        L = self.vehicle.get_distancia_eixo_traseiro_trailer_quinta_roda()\n",
    "        a = self.vehicle.get_distancia_eixo_traseiro_quinta_roda()\n",
    "\n",
    "        angular_velocity_tractor = (velocity / D) * tan(alpha)\n",
    "\n",
    "        # Kinematics\n",
    "        x_dot = velocity * cos(theta)\n",
    "        y_dot = velocity * sin(theta)\n",
    "        theta_dot = (velocity / D) * tan(alpha)\n",
    "        beta_dot = angular_velocity_tractor * (1 - (a * cos(beta)) / L) - (velocity * sin(beta)) / L\n",
    "\n",
    "        # Euler step\n",
    "        new_x = x + x_dot * dt\n",
    "        new_y = y + y_dot * dt\n",
    "        new_theta = theta + theta_dot * dt\n",
    "        new_beta = beta + beta_dot * dt\n",
    "\n",
    "        self.vehicle.update_physical_properties(new_x, new_y, velocity, new_theta, new_beta, alpha)\n",
    "        self.vehicle.update_raycasts(self.map.get_entities())\n",
    "\n",
    "    def _check_vehicle_collision(self) -> bool:\n",
    "        \"\"\"Verifica se o veículo colidiu com alguma parede ou passou por cima de uma vaga de estacionamento.\"\"\"\n",
    "        for entity in self.map.get_entities():\n",
    "            if entity.type == MapEntity.ENTITY_WALL or entity.type == MapEntity.ENTITY_PARKING_SLOT:\n",
    "                if self.vehicle.check_collision(entity):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def _calculate_goal_distance(self):\n",
    "        x, y = self.vehicle.get_position()\n",
    "        goal_x, goal_y = self.map.get_parking_goal_position()\n",
    "        distance_to_goal = np.sqrt((x - goal_x)**2 + (y - goal_y)**2)\n",
    "        return distance_to_goal\n",
    "\n",
    "    def _calculate_goal_distance_manhattan(self):\n",
    "        x, y = self.vehicle.get_position()\n",
    "        goal_x, goal_y = self.map.get_parking_goal_position()\n",
    "        distance_to_goal = abs(x - goal_x) + abs(y - goal_y)\n",
    "        return distance_to_goal\n",
    "\n",
    "    def _calculate_goal_direction(self):\n",
    "        x, y = self.vehicle.get_position()\n",
    "        goal_x, goal_y = self.map.get_parking_goal_position()\n",
    "        direction_to_goal = np.arctan2(goal_y - y, goal_x - x)\n",
    "        return direction_to_goal\n",
    "\n",
    "    def _check_vehicle_parking(self) -> bool:\n",
    "        \"\"\"Verifica se o trailer do veículo está dentro de uma vaga de estacionamento.\"\"\"\n",
    "        goal = self.map.get_parking_goal()\n",
    "        if goal.get_bounding_box().contains_bounding_box(self.vehicle.get_bounding_box_trailer()):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _check_trailer_jackknife(self) -> bool:\n",
    "        \"\"\"Verifica se o trailer do veículo está em jackknife.\"\"\"\n",
    "        return self.vehicle.get_beta() > np.deg2rad(65.0) or self.vehicle.get_beta() < np.deg2rad(-65.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b39b59",
   "metadata": {},
   "source": [
    "#### Funções de treinamento e avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: PPO, iterations: int = 10):\n",
    "    rewards = []\n",
    "    for _ in range(iterations):\n",
    "        rewards.append(run_episode(model, int(random.random() * 1000)))\n",
    "    return rewards\n",
    "    \n",
    "def run_episode_and_save_video(model):\n",
    "    video_recorder = Visualization.VideoRecorder(\"simulation.mp4\", fps=10)\n",
    "    env = ParkingEnv()\n",
    "    observation, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while(True):\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += float(reward)\n",
    "        video_recorder.append(env.render())\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    video_recorder.close()\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "def run_episode(model, seed = None):\n",
    "    env = ParkingEnv(seed)\n",
    "    observation, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while(True):\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += float(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "import platform\n",
    "\n",
    "N_ENVS = 6\n",
    "\n",
    "def make_env(seed: int = 0):\n",
    "    def _init():\n",
    "        env = ParkingEnv()\n",
    "        env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "        env.reset(seed=seed)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def make_vector_env(n_envs: int = N_ENVS, use_subproc: bool | None = None):\n",
    "    \"\"\"Create a vectorized environment.\n",
    "\n",
    "    On Windows (especially inside notebooks), `SubprocVecEnv` can be unstable\n",
    "    and cause BrokenPipeError/EOFError. There we default to `DummyVecEnv`.\n",
    "    \"\"\"\n",
    "    # Auto-select backend if not specified\n",
    "    if use_subproc is None:\n",
    "        use_subproc = platform.system() != \"Windows\"\n",
    "\n",
    "    env_fns = [make_env(seed=i) for i in range(n_envs)]\n",
    "\n",
    "    if use_subproc:\n",
    "        try:\n",
    "            return SubprocVecEnv(env_fns)\n",
    "        except Exception as e:\n",
    "            print(f\"SubprocVecEnv failed ({e}), falling back to DummyVecEnv.\")\n",
    "\n",
    "    return DummyVecEnv(env_fns)\n",
    "\n",
    "def train_ppo(model: PPO | None = None, total_timesteps: int = 10000, save_every: int | None = None, save_path: str | None = None, save_name: str | None = None):\n",
    "    \"\"\"\n",
    "        Treina o modelo PPO\n",
    "        Args:\n",
    "            model: Modelo PPO a ser treinado, se None, cria um novo modelo, senão treina modelo existente\n",
    "            total_timesteps: Total de timesteps para treinar\n",
    "            save_every: O modelo será salvo a cada save_every timesteps durante o treinamento\n",
    "            save_path: Caminho para o diretório onde o modelo será salvo\n",
    "            save_name: Nome do modelo\n",
    "        Returns:\n",
    "            Modelo PPO treinado\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = make_vector_env()\n",
    "\n",
    "    # neural network hyperparameters\n",
    "    # net_arch is a list of number of neurons per hidden layer, e.g. [16,20] means\n",
    "    # two hidden layers with 16 and 20 neurons, respectively\n",
    "    policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
    "                     net_arch=dict(pi=[64, 64], vf=[64, 64]))\n",
    "\n",
    "    # instantiates the model using the defined hyperparameters\n",
    "    if model is None:\n",
    "        model = PPO(\n",
    "            policy=\"MlpPolicy\",           # neural network policy architecture (MLP for vector observations)\n",
    "            env=env,                      # gymnasium-compatible environment to train on\n",
    "            policy_kwargs=policy_kwargs,  # custom network architecture and activation\n",
    "            verbose=0,                    # logging verbosity: 0(silent),1(info),2(debug)\n",
    "            tensorboard_log=log_dir,      # directory for TensorBoard logs\n",
    "            learning_rate=0.0003,           # optimizer learning rate\n",
    "            n_steps=2048,                 # rollout steps per environment update\n",
    "            batch_size=64,                # minibatch size for optimization\n",
    "            gamma=0.9995,                   # discount factor\n",
    "            gae_lambda=0.95,              # GAE lambda for bias-variance tradeoff\n",
    "            ent_coef=0.0,                 # entropy coefficient (encourages exploration)\n",
    "            clip_range=0.2,               # PPO clipping parameter\n",
    "            n_epochs=10,                  # number of optimization epochs per update\n",
    "\n",
    "            device=\"auto\"                 # use GPU if available, else CPU\n",
    "        )\n",
    "    else:\n",
    "        model.set_env(env)\n",
    "\n",
    "    # You can also experiment with other RL algorithms like A2C, PPO, DDPG etc.\n",
    "    # Refer to  https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
    "    # for documentation. For example, if you would like to run DDPG, just replace \"DQN\" above with \"DDPG\".\n",
    "    timesteps_split = 0\n",
    "    if save_every is not None:\n",
    "        timesteps_split = int(total_timesteps / save_every)\n",
    "        for i in range(timesteps_split):\n",
    "            model.learn(total_timesteps=save_every, progress_bar=True, tb_log_name=save_name, reset_num_timesteps=False,)\n",
    "            model_save_dir = os.path.join(save_path, save_name)\n",
    "            model.save(model_save_dir)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087c839",
   "metadata": {},
   "source": [
    "#### Funções auxiliares para visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Play Video function\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import platform\n",
    "\n",
    "# Only import and use pyvirtualdisplay on Linux\n",
    "if platform.system() != 'Windows':\n",
    "    from pyvirtualdisplay import Display\n",
    "else:\n",
    "    Display = None\n",
    "\n",
    "# create the directory to store the video(s)\n",
    "os.makedirs(\"./video\", exist_ok=True)\n",
    "\n",
    "# Only start virtual display on Linux (not needed on Windows)\n",
    "display = None\n",
    "if platform.system() != 'Windows' and Display is not None:\n",
    "    display = Display(visible=False, size=(2000, 1500))\n",
    "    _ = display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment\n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "def render_mp4(videopath: str) -> str:\n",
    "  \"\"\"\n",
    "  Gets a string containing a b4-encoded version of the MP4 video\n",
    "  at the specified path.\n",
    "  \"\"\"\n",
    "  if not os.path.exists(videopath):\n",
    "      return f'<p>Video file not found: {videopath}</p>'\n",
    "  mp4 = open(videopath, 'rb').read()\n",
    "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
    "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
    "\n",
    "def record_and_display_video_manual(env, model, video_name, num_episodes=1):\n",
    "    \"\"\"\n",
    "    Records a video manually using Visualization.VideoRecorder (more reliable).\n",
    "    \n",
    "    Args:\n",
    "        env: The gymnasium environment.\n",
    "        model: The trained model.\n",
    "        video_name (str): The name to use for the video file.\n",
    "        num_episodes (int): The number of episodes to record (default is 1).\n",
    "    \"\"\"\n",
    "    os.makedirs(\"./video\", exist_ok=True)\n",
    "    \n",
    "    video_path = f\"video/{video_name}.mp4\"\n",
    "    video_recorder = Visualization.VideoRecorder(video_path, fps=10)\n",
    "    \n",
    "    total_reward = 0.0\n",
    "    episode_count = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        observation, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        while(True):\n",
    "            action, _ = model.predict(observation, deterministic=True)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += float(reward)\n",
    "            video_recorder.append(env.render())\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        total_reward += episode_reward\n",
    "        episode_count += 1\n",
    "    \n",
    "    video_recorder.close()\n",
    "    print(f\"\\nTotal reward: {total_reward}\")\n",
    "    print(f\"Video saved to: {video_path}\")\n",
    "    \n",
    "    html = render_mp4(video_path)\n",
    "    return HTML(html)\n",
    "\n",
    "def record_and_display_video(env, model, video_name, num_episodes=1):\n",
    "    \"\"\"\n",
    "    Records a video of the agent performing in the environment and displays it.\n",
    "\n",
    "    Args:\n",
    "        env: The gymnasium environment.\n",
    "        model: The trained model.\n",
    "        video_name (str): The name to use for the video file.\n",
    "        num_episodes (int): The number of episodes to record (default is 1).\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    # create the directory to store the video(s)\n",
    "    os.makedirs(\"./video\", exist_ok=True)\n",
    "\n",
    "    # Use a virtual display for rendering (only on Linux)\n",
    "    display = None\n",
    "    if platform.system() != 'Windows' and Display is not None:\n",
    "        display = Display(visible=False, size=(1400, 900))\n",
    "        _ = display.start()\n",
    "\n",
    "    env_name = \"ParkingEnv\"\n",
    "\n",
    "    env = gym.wrappers.RecordVideo(\n",
    "        env,\n",
    "        video_folder=\"video\",\n",
    "        name_prefix=f\"{env_name}_{video_name}\",\n",
    "        episode_trigger=lambda episode_id: episode_id < num_episodes\n",
    "    )\n",
    "\n",
    "    observation, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    episode_count = 0\n",
    "\n",
    "    while not done:\n",
    "        action, states = model.predict(observation, deterministic=True)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            episode_count += 1\n",
    "            if episode_count < num_episodes:\n",
    "                observation, _ = env.reset()\n",
    "                done = False\n",
    "\n",
    "    env.close()\n",
    "    # Stop the virtual display if it was started\n",
    "    if display is not None:\n",
    "        display.stop()\n",
    "\n",
    "    print(f\"\\nTotal reward: {total_reward}\")\n",
    "\n",
    "    # Find the video file that was created\n",
    "    video_pattern = f\"video/{env_name}_{video_name}*.mp4\"\n",
    "    video_files = glob.glob(video_pattern)\n",
    "    \n",
    "    if not video_files:\n",
    "        # Try alternative pattern\n",
    "        video_pattern = f\"video/*{video_name}*.mp4\"\n",
    "        video_files = glob.glob(video_pattern)\n",
    "    \n",
    "    if not video_files:\n",
    "        # List all video files for debugging\n",
    "        all_videos = glob.glob(\"video/*.mp4\")\n",
    "        print(f\"Warning: Expected video file not found. Available video files: {all_videos}\")\n",
    "        return HTML(\"<p>Video file not found. Check the video directory.</p>\")\n",
    "    \n",
    "    # Use the first matching video file\n",
    "    video_path = video_files[0]\n",
    "    print(f\"Found video file: {video_path}\")\n",
    "    \n",
    "    # show video\n",
    "    html = render_mp4(video_path)\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def clean_logs():\n",
    "    if os.path.exists(log_dir):\n",
    "        print(\"Cleaning logs\")\n",
    "        shutil.rmtree(log_dir)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ppo_model_16_11\"\n",
    "model_save_dir = \"models\"\n",
    "total_training_timesteps = 10000000\n",
    "save_every = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b55e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "#se modelo salvo já existe, carrega\n",
    "if(os.path.exists(os.path.join(model_save_dir, model_name + \".zip\"))):\n",
    "    model_save_path = os.path.join(model_save_dir, model_name + \".zip\")\n",
    "    model = PPO.load(model_save_path)\n",
    "else: #senão, cria novo\n",
    "    model = None\n",
    "\n",
    "model = train_ppo(model, total_timesteps=total_training_timesteps, save_every=save_every, save_path=model_save_dir, save_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64dfe12",
   "metadata": {},
   "source": [
    "#### Carregar modelo já existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f97158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ppo_model_16_11\"\n",
    "\n",
    "model_save_path = os.path.join(model_save_dir, model_name + \".zip\")\n",
    "model = PPO.load(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ParkingEnv()\n",
    "record_and_display_video_manual(env, model, \"ppo_model\", num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accdf4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = evaluate_model(model, 100)\n",
    "total_rewards = np.array(total_rewards)\n",
    "print(f\"mean reward {total_rewards.mean()}\")\n",
    "print(f\"std reward {total_rewards.std()}\")\n",
    "print(f\"min reward {total_rewards.min()}\")\n",
    "print(f\"max reward {total_rewards.max()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
