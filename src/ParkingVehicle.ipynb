{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e72eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imageio imageio-ffmpeg pygame numerize pathlib casadi stable-baselines3 tensorboard \"stable-baselines3[extra]\"  pyvirtualdisplay ipywidgets --quiet\n",
    "%pip install \"gymnasium[other]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import stable_baselines3\n",
    "from SimulationConfigLoader import VehicleConfigLoader, MapConfigLoader\n",
    "from Simulation import ArticulatedVehicle\n",
    "from Simulation import Map\n",
    "from Simulation import MapEntity\n",
    "from ParkingEnv import ParkingEnv\n",
    "import random\n",
    "import Visualization\n",
    "from casadi import cos, sin, tan\n",
    "from typing import Any, SupportsFloat\n",
    "from stable_baselines3 import PPO, SAC\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "from numerize import numerize\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "import platform\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b39b59",
   "metadata": {},
   "source": [
    "#### Funções de treinamento e avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: PPO, iterations: int = 10):\n",
    "    rewards = []\n",
    "    for _ in range(iterations):\n",
    "        rewards.append(run_episode(model, int(random.random() * 1000)))\n",
    "    return rewards\n",
    "    \n",
    "def run_episode_and_save_video(model):\n",
    "    video_recorder = Visualization.VideoRecorder(\"simulation.mp4\", fps=10)\n",
    "    env = ParkingEnv()\n",
    "    observation, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while(True):\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += float(reward)\n",
    "        video_recorder.append(env.render())\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    video_recorder.close()\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "def run_episode(model, seed = None):\n",
    "    env = ParkingEnv(seed)\n",
    "    observation, info = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while(True):\n",
    "        action, _ = model.predict(observation, deterministic=True)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += float(reward)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "import platform\n",
    "\n",
    "N_ENVS = 8\n",
    "\n",
    "def make_env(seed: int = 0):\n",
    "    def _init():\n",
    "        env = ParkingEnv()\n",
    "        env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
    "        env.reset(seed=seed)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def make_vector_env(n_envs: int = N_ENVS, use_subproc: bool | None = None):\n",
    "    \"\"\"Create a vectorized environment.\n",
    "\n",
    "    On Windows (especially inside notebooks), `SubprocVecEnv` can be unstable\n",
    "    and cause BrokenPipeError/EOFError. There we default to `DummyVecEnv`.\n",
    "    \"\"\"\n",
    "    # Auto-select backend if not specified\n",
    "    if use_subproc is None:\n",
    "        use_subproc = platform.system() != \"Windows\"\n",
    "\n",
    "    env_fns = [make_env(seed=i) for i in range(n_envs)]\n",
    "\n",
    "    if use_subproc:\n",
    "        try:\n",
    "            return SubprocVecEnv(env_fns)\n",
    "        except Exception as e:\n",
    "            print(f\"SubprocVecEnv failed ({e}), falling back to DummyVecEnv.\")\n",
    "\n",
    "    return DummyVecEnv(env_fns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_sac(model: SAC | None = None, total_timesteps: int = 10000, save_every: int | None = None, save_path: str | None = None, save_name: str | None = None):\n",
    "    \"\"\"\n",
    "        Treina o modelo SAC (Soft Actor-Critic)\n",
    "        Args:\n",
    "            model: Modelo SAC a ser treinado, se None, cria um novo modelo, senão treina modelo existente\n",
    "            total_timesteps: Total de timesteps para treinar\n",
    "            save_every: O modelo será salvo a cada save_every timesteps durante o treinamento\n",
    "            save_path: Caminho para o diretório onde o modelo será salvo\n",
    "            save_name: Nome do modelo\n",
    "        Returns:\n",
    "            Modelo SAC treinado\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = make_vector_env()\n",
    "\n",
    "    # Hiperparâmetros da rede neural\n",
    "    # O README descreve um espaço de observação com 19 dimensões\n",
    "    # (3 ângulos, 14 raycasts, 2 de meta).\n",
    "    # Aumentamos a rede para [256, 256] para lidar com essa complexidade.\n",
    "    # No SAC (Actor-Critic), temos redes 'pi' (policy/ator) e 'qf' (Q-function/crítico).\n",
    "    policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
    "                         net_arch=dict(pi=[128, 128], qf=[256, 256]))\n",
    "\n",
    "    # Instancia o modelo SAC usando os hiperparâmetros definidos\n",
    "    if model is None:\n",
    "        model = SAC(\n",
    "            policy=\"MlpPolicy\",          # Arquitetura da política (MLP para observações vetoriais)\n",
    "            env=env,                     # Ambiente compatível com Gymnasium\n",
    "            policy_kwargs=policy_kwargs, # Arquitetura de rede customizada\n",
    "            verbose=0,                   # Verbosidade do log\n",
    "            tensorboard_log=log_dir,     # Diretório para logs do TensorBoard\n",
    "            learning_rate=0.0003,        # Taxa de aprendizado (um bom padrão)\n",
    "            buffer_size=1000000,          # Tamanho do replay buffer (SAC é off-policy)\n",
    "            batch_size=256,              # Tamanho do batch amostrado do buffer\n",
    "            gamma=0.99,                # Fator de desconto (mantido alto, pois estacionar é uma tarefa de horizonte longo)\n",
    "            ent_coef=\"auto\",             # Coeficiente de entropia (essencial no SAC, 'auto' aprende automaticamente)\n",
    "            tau=0.005,                   # Coeficiente de \"soft update\" para as redes de target\n",
    "            learning_starts=10000,        # Número de passos antes de começar a treinar (coleta experiência)\n",
    "            device=\"auto\",            # Usa GPU se disponível\n",
    "            use_sde=True,\n",
    "\n",
    "        )\n",
    "    else:\n",
    "        model.set_env(env)\n",
    "\n",
    "    timesteps_split = 0\n",
    "    if save_every is not None:\n",
    "        timesteps_split = int(total_timesteps / save_every)\n",
    "        for i in range(timesteps_split):\n",
    "            model.learn(total_timesteps=save_every, progress_bar=True, tb_log_name=save_name, reset_num_timesteps=False,)\n",
    "            model_save_dir = os.path.join(save_path, save_name)\n",
    "            model.save(model_save_dir)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_PPO(model: PPO | None = None, total_timesteps: int = 10000, save_every: int | None = None, save_path: str | None = None, save_name: str | None = None):\n",
    "    \"\"\"\n",
    "        Treina o modelo PPO (Proximal Policy Optimization)\n",
    "        Args:\n",
    "            model: Modelo PPO a ser treinado, se None, cria um novo modelo, senão treina modelo existente\n",
    "            total_timesteps: Total de timesteps para treinar\n",
    "            save_every: O modelo será salvo a cada save_every timesteps durante o treinamento\n",
    "            save_path: Caminho para o diretório onde o modelo será salvo\n",
    "            save_name: Nome do modelo\n",
    "        Returns:\n",
    "            Modelo SAC treinado\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = make_vector_env()\n",
    "    # neural network hyperparameters\n",
    "    # net_arch is a list of number of neurons per hidden layer, e.g. [16,20] means\n",
    "    # two hidden layers with 16 and 20 neurons, respectively\n",
    "    policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
    "                     net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "\n",
    "    # instantiates the model using the defined hyperparameters\n",
    "    if model is None:\n",
    "        model = PPO(\n",
    "            policy=\"MlpPolicy\",           # neural network policy architecture (MLP for vector observations)\n",
    "            env=env,                      # gymnasium-compatible environment to train on\n",
    "            policy_kwargs=policy_kwargs,  # custom network architecture and activation\n",
    "            verbose=0,                    # logging verbosity: 0(silent),1(info),2(debug)\n",
    "            tensorboard_log=log_dir,      # directory for TensorBoard logs\n",
    "            learning_rate=0.0003,           # optimizer learning rate\n",
    "            n_steps=2048,                 # rollout steps per environment update\n",
    "            batch_size=64,                # minibatch size for optimization\n",
    "            gamma=0.999,                   # discount factor\n",
    "            gae_lambda=0.95,              # GAE lambda for bias-variance tradeoff\n",
    "            ent_coef=0.0,                 # entropy coefficient (encourages exploration)\n",
    "            clip_range=0.2,               # PPO clipping parameter\n",
    "            n_epochs=10,                  # number of optimization epochs per update\n",
    "            device=\"auto\"                 # use GPU if available, else CPU\n",
    "        )\n",
    "    else:\n",
    "\n",
    "        model.set_env(env)\n",
    "\n",
    "    timesteps_split = 0\n",
    "    if save_every is not None:\n",
    "        timesteps_split = int(total_timesteps / save_every)\n",
    "        for i in range(timesteps_split):\n",
    "            model.learn(total_timesteps=save_every, progress_bar=True, tb_log_name=save_name, reset_num_timesteps=False,)\n",
    "            model_save_dir = os.path.join(save_path, save_name)\n",
    "            model.save(model_save_dir)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087c839",
   "metadata": {},
   "source": [
    "#### Funções auxiliares para visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Play Video function\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import platform\n",
    "\n",
    "# Only import and use pyvirtualdisplay on Linux\n",
    "if platform.system() != 'Windows':\n",
    "    from pyvirtualdisplay import Display\n",
    "else:\n",
    "    Display = None\n",
    "\n",
    "# create the directory to store the video(s)\n",
    "os.makedirs(\"./video\", exist_ok=True)\n",
    "\n",
    "# Only start virtual display on Linux (not needed on Windows)\n",
    "display = None\n",
    "if platform.system() != 'Windows' and Display is not None:\n",
    "    display = Display(visible=False, size=(2000, 1500))\n",
    "    _ = display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment\n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "def render_mp4(videopath: str) -> str:\n",
    "  \"\"\"\n",
    "  Gets a string containing a b4-encoded version of the MP4 video\n",
    "  at the specified path.\n",
    "  \"\"\"\n",
    "  if not os.path.exists(videopath):\n",
    "      return f'<p>Video file not found: {videopath}</p>'\n",
    "  mp4 = open(videopath, 'rb').read()\n",
    "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
    "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
    "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'\n",
    "\n",
    "def record_and_display_video_manual(env, model, video_name, num_episodes=1):\n",
    "    \"\"\"\n",
    "    Records a video manually using Visualization.VideoRecorder (more reliable).\n",
    "    \n",
    "    Args:\n",
    "        env: The gymnasium environment.\n",
    "        model: The trained model.\n",
    "        video_name (str): The name to use for the video file.\n",
    "        num_episodes (int): The number of episodes to record (default is 1).\n",
    "    \"\"\"\n",
    "    os.makedirs(\"./video\", exist_ok=True)\n",
    "    \n",
    "    video_path = f\"video/{video_name}.mp4\"\n",
    "    video_recorder = Visualization.VideoRecorder(video_path, fps=10)\n",
    "    \n",
    "    total_reward = 0.0\n",
    "    episode_count = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        observation, info = env.reset()\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        while(True):\n",
    "            action, _ = model.predict(observation, deterministic=True)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += float(reward)\n",
    "            video_recorder.append(env.render())\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        total_reward += episode_reward\n",
    "        episode_count += 1\n",
    "    \n",
    "    video_recorder.close()\n",
    "    print(f\"\\nTotal reward: {total_reward}\")\n",
    "    print(f\"Video saved to: {video_path}\")\n",
    "    \n",
    "    html = render_mp4(video_path)\n",
    "    return HTML(html)\n",
    "\n",
    "def record_and_display_video(env, model, video_name, num_episodes=1):\n",
    "    \"\"\"\n",
    "    Records a video of the agent performing in the environment and displays it.\n",
    "\n",
    "    Args:\n",
    "        env: The gymnasium environment.\n",
    "        model: The trained model.\n",
    "        video_name (str): The name to use for the video file.\n",
    "        num_episodes (int): The number of episodes to record (default is 1).\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    # create the directory to store the video(s)\n",
    "    os.makedirs(\"./video\", exist_ok=True)\n",
    "\n",
    "    # Use a virtual display for rendering (only on Linux)\n",
    "    display = None\n",
    "    if platform.system() != 'Windows' and Display is not None:\n",
    "        display = Display(visible=False, size=(1400, 900))\n",
    "        _ = display.start()\n",
    "\n",
    "    env_name = \"ParkingEnv\"\n",
    "\n",
    "    env = gym.wrappers.RecordVideo(\n",
    "        env,\n",
    "        video_folder=\"video\",\n",
    "        name_prefix=f\"{env_name}_{video_name}\",\n",
    "        episode_trigger=lambda episode_id: episode_id < num_episodes\n",
    "    )\n",
    "\n",
    "    observation, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    episode_count = 0\n",
    "\n",
    "    while not done:\n",
    "        action, states = model.predict(observation, deterministic=True)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            episode_count += 1\n",
    "            if episode_count < num_episodes:\n",
    "                observation, _ = env.reset()\n",
    "                done = False\n",
    "\n",
    "    env.close()\n",
    "    # Stop the virtual display if it was started\n",
    "    if display is not None:\n",
    "        display.stop()\n",
    "\n",
    "    print(f\"\\nTotal reward: {total_reward}\")\n",
    "\n",
    "    # Find the video file that was created\n",
    "    video_pattern = f\"video/{env_name}_{video_name}*.mp4\"\n",
    "    video_files = glob.glob(video_pattern)\n",
    "    \n",
    "    if not video_files:\n",
    "        # Try alternative pattern\n",
    "        video_pattern = f\"video/*{video_name}*.mp4\"\n",
    "        video_files = glob.glob(video_pattern)\n",
    "    \n",
    "    if not video_files:\n",
    "        # List all video files for debugging\n",
    "        all_videos = glob.glob(\"video/*.mp4\")\n",
    "        print(f\"Warning: Expected video file not found. Available video files: {all_videos}\")\n",
    "        return HTML(\"<p>Video file not found. Check the video directory.</p>\")\n",
    "    \n",
    "    # Use the first matching video file\n",
    "    video_path = video_files[0]\n",
    "    print(f\"Found video file: {video_path}\")\n",
    "    \n",
    "    # show video\n",
    "    html = render_mp4(video_path)\n",
    "    return HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def clean_logs():\n",
    "    if os.path.exists(log_dir):\n",
    "        print(\"Cleaning logs\")\n",
    "        shutil.rmtree(log_dir)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f04c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SAC_19_11_V2\"\n",
    "model_save_dir = \"models\"\n",
    "total_training_timesteps = 100000000\n",
    "save_every = 100000\n",
    "\n",
    "os.makedirs(model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b55e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#se modelo salvo já existe, carrega\n",
    "if(os.path.exists(os.path.join(model_save_dir, model_name + \".zip\"))):\n",
    "    model_save_path = os.path.join(model_save_dir, model_name + \".zip\")\n",
    "    model = SAC.load(model_save_path)\n",
    "else: #senão, cria novo\n",
    "    model = None\n",
    "\n",
    "model = train_sac(model, total_timesteps=total_training_timesteps, save_every=save_every, save_path=model_save_dir, save_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64dfe12",
   "metadata": {},
   "source": [
    "#### Carregar modelo já existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f97158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = os.path.join(model_save_dir, model_name + \".zip\")\n",
    "model = SAC.load(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ParkingEnv()\n",
    "record_and_display_video_manual(env, model, \"ppo_model\", num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accdf4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = evaluate_model(model, 10)\n",
    "total_rewards = np.array(total_rewards)\n",
    "print(f\"mean reward {total_rewards.mean()}\")\n",
    "print(f\"std reward {total_rewards.std()}\")\n",
    "print(f\"min reward {total_rewards.min()}\")\n",
    "print(f\"max reward {total_rewards.max()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
